{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2965c631",
   "metadata": {},
   "source": [
    "# Data preparation for Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46f75dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.mode.chained_assignment = None # Pretty sure this will come back to stab me in the back\n",
    "\n",
    "githubrepo = 'https://raw.githubusercontent.com/joedav98/SC1015_SC18_SpotifyRepo/main/data/'\n",
    "# Data is uploaded to a github repo, in order to pull when we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52116968",
   "metadata": {},
   "source": [
    "### Collection of functions for dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46c92c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converttonumeric(final):\n",
    "    final.danceability = pd.to_numeric(final.danceability)\n",
    "    final.energy = pd.to_numeric(final.energy)\n",
    "    final.loudness = pd.to_numeric(final.loudness)\n",
    "    final.speechiness = pd.to_numeric(final.speechiness)\n",
    "    final.acousticness = pd.to_numeric(final.acousticness)\n",
    "    final.liveness = pd.to_numeric(final.liveness)\n",
    "    final.valence = pd.to_numeric(final.valence)\n",
    "    final.tempo = pd.to_numeric(final.tempo)\n",
    "    final.duration_ms = pd.to_numeric(final.duration_ms)\n",
    "    final.year = pd.to_numeric(final.year)\n",
    "# Function to convert all values to numeric\n",
    "\n",
    "def convertKeytoCat(df):\n",
    "    df['key'] = df['key'].replace({\n",
    "      0 : 'C', \n",
    "      1 : 'C#/Db', \n",
    "      2 : 'D', \n",
    "      3 : 'D#/Eb', \n",
    "      4 : 'E', \n",
    "      5 : 'F', \n",
    "      6 : 'F#/Gb', \n",
    "      7 : 'G', \n",
    "      8 : 'G#/Ab', \n",
    "      9 : 'A', \n",
    "      10 : 'A#/Bb', \n",
    "      11 : 'B'})\n",
    "# Convert key from numbers to letters\n",
    "  \n",
    "def convertKeytoNum(df):\n",
    "    drop20s['key'] = drop20s['key'].str[0].map({\n",
    "      'C' : 0, \n",
    "      'C#/Db' : 1, \n",
    "      'D' : 2, \n",
    "      'D#/Eb' : 3, \n",
    "      'E' : 4, \n",
    "      'F' : 5, \n",
    "      'F#/Gb' : 6, \n",
    "      'G' : 7, \n",
    "      'G#/Ab' : 8, \n",
    "      'A' : 9, \n",
    "      'A#/Bb' : 10, \n",
    "      'B' : 11})\n",
    "# Encode key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d327c3",
   "metadata": {},
   "source": [
    "## NonHit songs dataset\n",
    "\n",
    "Taken from https://www.kaggle.com/datasets/luckey01/test-data-set\n",
    "\n",
    "The original CSV files had to be split into separate files to upload to GitHub, so they are being recombined in the top of the code. Each decade had their own number of hit songs, thus an equal amount for each decade was sampled from the nonhits. Decade10 encountered some problems as there was a heavy skew towards the later years, thus they were manually sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5af2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonhit0 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_0.csv')\n",
    "nonhit1 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_1.csv')\n",
    "nonhit2 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_2.csv')\n",
    "nonhit3 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_3.csv')\n",
    "nonhit4 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_4.csv')\n",
    "nonhit5 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_5.csv')\n",
    "nonhit6 = pd.read_csv(githubrepo + 'spotify_tracks_metadata_6.csv')\n",
    "\n",
    "\n",
    "nonhit1 = pd.DataFrame(data = nonhit1.values, columns = nonhit0.columns)\n",
    "nonhit2 = pd.DataFrame(data = nonhit2.values, columns = nonhit0.columns)\n",
    "nonhit3 = pd.DataFrame(data = nonhit3.values, columns = nonhit0.columns)\n",
    "nonhit4 = pd.DataFrame(data = nonhit4.values, columns = nonhit0.columns)\n",
    "nonhit5 = pd.DataFrame(data = nonhit5.values, columns = nonhit0.columns)\n",
    "nonhit6 = pd.DataFrame(data = nonhit6.values, columns = nonhit0.columns)\n",
    "\n",
    "\n",
    "nonhitmerge = pd.concat([nonhit0, nonhit1, nonhit2, nonhit3, nonhit4, nonhit5, nonhit6], ignore_index = True).sort_values(by = ['album_release_year'])\n",
    "\n",
    "nonhitpre = nonhitmerge.drop(labels = ['Unnamed: 0',\n",
    "                                        'spotify_id',\n",
    "                                        'album_release_date',\n",
    "                                        'album_release_month',\n",
    "                                        'analysis_url',\n",
    "                                        'mode',\n",
    "                                        'song_explicit',\n",
    "                                        'time_signature',\n",
    "                                        'total_available_markets',\n",
    "                                        'track_href',\n",
    "                                        'uri',\n",
    "                                        'instrumentalness'],\n",
    "                      axis = 1,\n",
    "                      inplace = False)\n",
    "\n",
    "nonhitpre.rename(columns = {'song_name':'track',\n",
    "                            'artist_name':'artist',\n",
    "                            'album_release_year':'year'},\n",
    "               inplace = True) # Renaming columns to merge with larger dataset\n",
    "\n",
    "nonhitprocess = nonhitpre[nonhitpre.song_popularity <= 50].dropna() # Using popularity feature to avoid hit songs\n",
    "nonhitprocess = nonhitprocess.drop(labels = ['song_popularity'], axis = 1, inplace = False)\n",
    "\n",
    "decade90nonhitpre = nonhitprocess[(nonhitprocess.year >= 1990) & (nonhitprocess.year <= 1999)].sort_values(by = ['year'])\n",
    "decade00nonhitpre = nonhitprocess[(nonhitprocess.year >= 2000) & (nonhitprocess.year <= 2009)].sort_values(by = ['year'])\n",
    "decade10nonhitpre1 = nonhitprocess[(nonhitprocess.year >= 2010) & (nonhitprocess.year <= 2018)].sort_values(by = ['year'])\n",
    "decade10nonhitpre2 = nonhitprocess[(nonhitprocess.year == 2019)].sort_values(by = ['year'])\n",
    "decade10nonhitpre3 = nonhitprocess[(nonhitprocess.year == 2020)].sort_values(by = ['year'])\n",
    "decade10nonhitpre4 = nonhitprocess[(nonhitprocess.year == 2021)].sort_values(by = ['year'])\n",
    "\n",
    "decade90nonhit = decade90nonhitpre.sample(n = 2700, random_state = 52)\n",
    "decade00nonhit = decade00nonhitpre.sample(n = 2830, random_state = 52)\n",
    "decade10nonhit1 = decade10nonhitpre1.sample(n = 2570, random_state = 52)\n",
    "decade10nonhit2 = decade10nonhitpre2.sample(n = 480, random_state = 52)\n",
    "decade10nonhit3 = decade10nonhitpre3.sample(n = 450, random_state = 52)\n",
    "decade10nonhit4 = decade10nonhitpre4.sample(n = 500, random_state = 52)\n",
    "\n",
    "decade10nonhit = pd.concat([decade10nonhit1, decade10nonhit2, decade10nonhit3, decade10nonhit4], join = 'inner')\n",
    "\n",
    "for nonhit in [decade90nonhit, decade00nonhit, decade10nonhit]:\n",
    "    nonhit['charted'] = False\n",
    "    nonhit['weeks-on-board'] = 0\n",
    "    converttonumeric(nonhit)\n",
    "    convertKeytoCat(nonhit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6411906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade 90 NonHits: 2700\n",
      "Decade 00 NonHits: 2830\n",
      "Decade 10 NonHits: 4000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decade 90 NonHits: {len(decade90nonhit.index)}\")\n",
    "print(f\"Decade 00 NonHits: {len(decade00nonhit.index)}\")\n",
    "print(f\"Decade 10 NonHits: {len(decade10nonhit.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e0b9c",
   "metadata": {},
   "source": [
    "## Hit songs from 2020 and 2021\n",
    "Taken from https://www.kaggle.com/datasets/sashankpillai/spotify-top-200-charts-20202021\n",
    "\n",
    "The original hit songs dataset that we acquired lacked data from 2020 to 2021, thus we had to find a separate dataset to fill in the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492e6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data20s = pd.read_csv(githubrepo + 'dataset-of-20s.csv')\n",
    "data20s = data20s[data20s['Highest Charting Position'] <= 100] # Getting only top 100, instead of top 200\n",
    "drop20s = data20s.drop(labels = ['Index',\n",
    "                                 'Highest Charting Position',\n",
    "                                 'Week of Highest Charting',\n",
    "                                 'Streams',\n",
    "                                 'Artist Followers',\n",
    "                                 'Song ID',\n",
    "                                 'Genre',\n",
    "                                 'Popularity',\n",
    "                                 'Release Date'],\n",
    "                       axis = 1,\n",
    "                       inplace = False) # Dataset of hit songs from 2019 to 2021\n",
    "\n",
    "drop20s['Weeks Charted'] = drop20s['Weeks Charted'].str[:4] # Extracting year of charting from weeks charted\n",
    "\n",
    "drop20s.rename(columns = {'Number of Times Charted':'weeks-on-board',\n",
    "                          'Song Name':'track',\n",
    "                          'Artist':'artist',\n",
    "                          'Weeks Charted':'year',\n",
    "                          'Danceability':'danceability',\n",
    "                          'Energy':'energy',\n",
    "                          'Loudness':'loudness',\n",
    "                          'Speechiness':'speechiness',\n",
    "                          'Acousticness':'acousticness',\n",
    "                          'Liveness':'liveness',\n",
    "                          'Tempo':'tempo',\n",
    "                          'Duration (ms)':'duration_ms',\n",
    "                          'Valence':'valence',\n",
    "                          'Chord':'key'},\n",
    "               inplace = True) # Renaming columns to merge with larger dataset\n",
    "\n",
    "drop20s = drop20s.replace(r'^\\s*$', np.nan, regex=True)\n",
    "drop20s.dropna(subset=['danceability'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa84b9",
   "metadata": {},
   "source": [
    "## Hit songs dataset\n",
    "Taken from \n",
    "\n",
    "https://www.kaggle.com/datasets/theoverman/the-spotify-hit-predictor-dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/dhruvildave/spotify-charts\n",
    "\n",
    "First dataset was used to get the song features necessary for our analysis. Only data from 1990 to 2021 was extracted. The second dataset contained our billboard charts data, which included the number of weeks a song has been on the billboard charts. We used this to convert to a boolean feature that simply states whether or not a song is a hit.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a215cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data90s = pd.read_csv(githubrepo + 'dataset-of-90s.csv')\n",
    "data00s = pd.read_csv(githubrepo + 'dataset-of-00s.csv')\n",
    "data10s = pd.read_csv(githubrepo + 'dataset-of-10s.csv')\n",
    "charts = pd.read_csv(githubrepo + 'charts.csv').drop_duplicates(subset=['song', 'artist'], keep = 'first') \n",
    "# Dataset from Billboard, drop duplicates leaves only the latest occurence of a hit song\n",
    "\n",
    "datayears = [data90s, data00s, data10s] # Dataset of top tracks from 1960s to 2010s, with Echonest info\n",
    "reference = pd.concat(datayears, ignore_index = True).drop_duplicates(subset = ['track', 'artist'], keep = 'last') # Leaves only latest occurence of a hit song\n",
    "charts['date'] = charts['date'].str[:-6]\n",
    "\n",
    "dropref = reference.drop(labels = ['uri', \n",
    "                                   'mode',\n",
    "                                   'chorus_hit',\n",
    "                                   'sections',\n",
    "                                   'target',\n",
    "                                   'instrumentalness',\n",
    "                                   'time_signature'], \n",
    "                        axis = 1, \n",
    "                        inplace = False) # Dropping features that won't be helpful in analysis\n",
    "\n",
    "dropcharts = charts.drop(labels = ['rank', \n",
    "                                   'last-week',\n",
    "                                   'peak-rank'],\n",
    "                         axis = 1,\n",
    "                         inplace = False).reset_index(drop = True) # Dropping features that won't be helpful in analysis\n",
    "# Rank and peak rank can change in a single year, last week is redundant as we are using the year it charted\n",
    "\n",
    "mergedDF = pd.merge(dropref, dropcharts, left_on = ['track', 'artist'], right_on = ['song', 'artist']) # Merging Spotify data with Billboard data\n",
    "\n",
    "mergedDF = mergedDF.drop(labels = ['song'], axis = 1, inplace = False)\n",
    "mergedDF.rename(columns = {'date':'year'}, inplace = True)\n",
    "\n",
    "mergedDF.year = pd.to_numeric(mergedDF.year)\n",
    "\n",
    "decade90hit = mergedDF[(mergedDF.year >= 1990) & (mergedDF.year <= 1999)]\n",
    "decade00hit = mergedDF[(mergedDF.year >= 2000) & (mergedDF.year <= 2009)]\n",
    "decade10hit = mergedDF[(mergedDF.year >= 2010) & (mergedDF.year <= 2021)]\n",
    "\n",
    "decade10hit = pd.concat([decade10hit, drop20s], join = 'outer').drop_duplicates(subset = ['track', 'artist'], keep = 'last') # Joining the merged dataset with the dataset from 2019 to 2021, defaulting to merged dataset for clashes\n",
    "\n",
    "hits = [decade90hit, decade00hit, decade10hit]\n",
    "for hit in hits:\n",
    "    hit['charted'] = True\n",
    "    converttonumeric(hit)\n",
    "    convertKeytoCat(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c284e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade 90 Hits: 2673\n",
      "Decade 00 Hits: 2831\n",
      "Decade 10 Hits: 4065\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decade 90 Hits: {len(decade90hit.index)}\")\n",
    "print(f\"Decade 00 Hits: {len(decade00hit.index)}\")\n",
    "print(f\"Decade 10 Hits: {len(decade10hit.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b7a39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decade90 = pd.concat([decade90hit, decade90nonhit], join = 'inner').reset_index(drop = True)\n",
    "decade00 = pd.concat([decade00hit, decade00nonhit], join = 'inner').reset_index(drop = True)\n",
    "decade10 = pd.concat([decade10hit, decade10nonhit], join = 'inner').reset_index(drop = True)\n",
    "\n",
    "decadeHits = pd.concat([decade90hit,decade00hit,decade10hit], join = 'inner').reset_index(drop = True)\n",
    "decadeNonHits = pd.concat([decade90nonhit,decade00nonhit,decade10nonhit], join = 'inner').reset_index(drop = True)\n",
    "\n",
    "# Sorting by year then by weeks-on-board, resetting index as a final preparation of the dataset\n",
    "decade90 = decade90.sort_values(by = ['year', 'weeks-on-board']).reset_index(drop = True)\n",
    "decade00 = decade00.sort_values(by = ['year', 'weeks-on-board']).reset_index(drop = True)\n",
    "decade10 = decade10.sort_values(by = ['year', 'weeks-on-board']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fecd7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "prep = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "\n",
    "# Normalising values with MinMaxScaler()\n",
    "\n",
    "for decade in [decade90, decade00, decade10]:\n",
    "    decade[prep] = scaler.fit_transform(decade[prep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d172965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_names = ['track', \n",
    "                'artist', \n",
    "                'key', \n",
    "                'danceability', \n",
    "                'energy', \n",
    "                'loudness', \n",
    "                'speechiness', \n",
    "                'acousticness', \n",
    "                'liveness', \n",
    "                'valence', \n",
    "                'tempo', \n",
    "                'duration_ms', \n",
    "                'year', \n",
    "                'weeks-on-board', \n",
    "                'charted'] \n",
    "\n",
    "decade90 = decade90.reindex(columns = column_names)\n",
    "decade00 = decade00.reindex(columns = column_names)\n",
    "decade10 = decade10.reindex(columns = column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e20ab",
   "metadata": {},
   "source": [
    "## Output main decades to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d3bdbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decade90nonhit.to_csv('output/eda/decade90nonhit.csv', index = False)\n",
    "decade00nonhit.to_csv('output/eda/decade00nonhit.csv', index = False)\n",
    "decade10nonhit.to_csv('output/eda/decade10nonhit.csv', index = False)\n",
    "\n",
    "decade90hit.to_csv('output/eda/decade90hit.csv', index = False)\n",
    "decade00hit.to_csv('output/eda/decade00hit.csv', index = False)\n",
    "decade10hit.to_csv('output/eda/decade10hit.csv', index = False)\n",
    "\n",
    "decade90.to_csv('output/decade90.csv', index = False)\n",
    "decade00.to_csv('output/decade00.csv', index = False)\n",
    "decade10.to_csv('output/decade10.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75c33c",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11413bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>weeks-on-board</th>\n",
       "      <th>charted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3861</th>\n",
       "      <td>0.701</td>\n",
       "      <td>0.688</td>\n",
       "      <td>E</td>\n",
       "      <td>-5.660</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.01940</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.445</td>\n",
       "      <td>97.684</td>\n",
       "      <td>287027.0</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.569</td>\n",
       "      <td>C#/Db</td>\n",
       "      <td>-10.386</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.02910</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.703</td>\n",
       "      <td>110.129</td>\n",
       "      <td>286627.0</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>0.388</td>\n",
       "      <td>0.978</td>\n",
       "      <td>E</td>\n",
       "      <td>-8.633</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.00209</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.321</td>\n",
       "      <td>109.325</td>\n",
       "      <td>186960.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.715</td>\n",
       "      <td>C#/Db</td>\n",
       "      <td>-12.071</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.02360</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.739</td>\n",
       "      <td>92.012</td>\n",
       "      <td>288627.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.650</td>\n",
       "      <td>F#/Gb</td>\n",
       "      <td>-8.004</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.30700</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.788</td>\n",
       "      <td>132.274</td>\n",
       "      <td>340867.0</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      danceability  energy    key  loudness  speechiness  acousticness  \\\n",
       "3861         0.701   0.688      E    -5.660       0.2110       0.01940   \n",
       "1109         0.824   0.569  C#/Db   -10.386       0.0348       0.02910   \n",
       "2344         0.388   0.978      E    -8.633       0.0750       0.00209   \n",
       "4279         0.571   0.715  C#/Db   -12.071       0.0422       0.02360   \n",
       "1638         0.756   0.650  F#/Gb    -8.004       0.0457       0.30700   \n",
       "\n",
       "      liveness  valence    tempo  duration_ms  weeks-on-board  charted  \n",
       "3861    0.0384    0.445   97.684     287027.0              13     True  \n",
       "1109    0.0595    0.703  110.129     286627.0              13     True  \n",
       "2344    0.2350    0.321  109.325     186960.0               0    False  \n",
       "4279    0.1580    0.739   92.012     288627.0               0    False  \n",
       "1638    0.0606    0.788  132.274     340867.0              20     True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#dropping certain features for each decade\n",
    "decade90_final = decade90.drop(labels = ['track', 'artist', 'year'], axis = 1, inplace = False)\n",
    "decade00_final = decade00.drop(labels = ['track', 'artist', 'year'], axis = 1, inplace = False)\n",
    "decade10_final = decade10.drop(labels = ['track', 'artist', 'year'], axis = 1, inplace = False)\n",
    "\n",
    "train90, test90 = train_test_split(decade90_final, test_size = 0.2, random_state = 52)\n",
    "train00, test00 = train_test_split(decade00_final, test_size = 0.2, random_state = 52)\n",
    "train10, test10 = train_test_split(decade10_final, test_size = 0.2, random_state = 52)\n",
    "\n",
    "train90.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e858dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and test sets for each year (Categorical)\n",
    "\n",
    "x_train90 = train90.drop(labels = ['weeks-on-board', 'charted', 'key'] , axis = 1)\n",
    "y_train90 = train90[['charted']]\n",
    "x_test90 = test90.drop(labels = ['weeks-on-board', 'charted', 'key'] , axis = 1)\n",
    "y_test90 = test90[['charted']]\n",
    "\n",
    "x_train00 = train00.drop(labels = ['weeks-on-board', 'charted', 'key'] , axis = 1)\n",
    "y_train00 = train00[['charted']]\n",
    "x_test00 = test00.drop(labels = ['weeks-on-board', 'charted', 'key'] , axis = 1)\n",
    "y_test00 = test00[['charted']]\n",
    "\n",
    "x_train10 = train10.drop(labels = ['weeks-on-board', 'charted', 'key'] , axis = 1)\n",
    "y_train10 = train10[['charted']]\n",
    "x_test10 = test10.drop(labels = ['weeks-on-board', 'charted', 'key'] , axis = 1)\n",
    "y_test10 = test10[['charted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724cef6b",
   "metadata": {},
   "source": [
    "## Output Train and Test datasets to CSVs\n",
    "\n",
    "To be used with spotify_machine_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41b6dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train90.to_csv('output/train/x_train90.csv', index = False)\n",
    "x_train00.to_csv('output/train/x_train00.csv', index = False)\n",
    "x_train10.to_csv('output/train/x_train10.csv', index = False)\n",
    "\n",
    "y_train90.to_csv('output/train/y_train90.csv', index = False)\n",
    "y_train00.to_csv('output/train/y_train00.csv', index = False)\n",
    "y_train10.to_csv('output/train/y_train10.csv', index = False)\n",
    "\n",
    "x_test90.to_csv('output/test/x_test90.csv', index = False)\n",
    "x_test00.to_csv('output/test/x_test00.csv', index = False)\n",
    "x_test10.to_csv('output/test/x_test10.csv', index = False)\n",
    "\n",
    "y_test90.to_csv('output/test/y_test90.csv', index = False)\n",
    "y_test00.to_csv('output/test/y_test00.csv', index = False)\n",
    "y_test10.to_csv('output/test/y_test10.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e859d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
